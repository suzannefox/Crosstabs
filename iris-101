
# source : https://www.r-bloggers.com/clustering-mixed-data-types-in-r/
# ========================================================
rm(list=ls())
set.seed(1680) # for reproducibility

library(dplyr) # for data cleaning
library(ggplot2) # for visualization
library(corrplot) # for correlations
library(caret) # for modelling
library(Boruta)

source('C:/Users/Suzanne/OneDrive - Suzanne Fox/R/myFunctions/R_CROSSTAB.R')
source('C:/Users/Suzanne/OneDrive - Suzanne Fox/R/myFunctions/burrow/R/R_DATA_SHAPE.R')

# ========================================================
# have a look at the data

data.in <- iris
data.in <- data.in %>%
  mutate(Petals=as.character(dplyr::ntile(Petal.Width, 4)))

data.shape <- datashape(data.in)

# get list of numeric variables
vars.nums <- data.shape %>%
  filter(DATA_TYPE %in% c("numeric")) %>%
  select(COLUMN_NAME) %>%
  pull()

# get dataframe with numerics only
data.nums <- data.in %>%
  select(vars.nums)

# correlations
data.cor <- cor(data.nums)
corrplot.mixed(data.cor)

# crosstabs
crosstab(data.in$Species, data.in$Petals)
crosstab(data.in$Petal.Length, data.in$Petals, 6)

# ========================================================
# test and train data

library(caret)
data.partition <- createDataPartition(data.in$Species, p = 0.8, list=FALSE)
data.train <- data.in[data.partition,]
data.test <- data.in[-data.partition,]

# ========================================================
# pre-process
# https://machinelearningmastery.com/pre-process-your-dataset-in-r/

# NORMALISE to 0-1
# calculate the pre-process parameters from the dataset
pre.processParams <- preProcess(iris[,1:4], method=c("range"))
# summarize transform parameters
print(pre.processParams)
# transform the dataset using the parameters
data.transformed <- predict(pre.processParams, iris[,1:4])
# summarize the transformed dataset
summary(data.transformed)

# CENTER, SCALE, PCA
# calculate the pre-process parameters from the dataset
pre.processParams <- preProcess(data.in, method=c("center", "scale", "pca"))
# summarize transform parameters
print(pre.processParams)
# transform the dataset using the parameters
data.transformed <- predict(pre.processParams, iris)
# summarize the transformed dataset
summary(data.transformed)

# ========================================================
# KNN 
# may need to normalise if range of values is different
# use max/min from shape
# make characters factors

data.knn.train <- data.train %>%
  select(-Petals)

data.knn.test <- data.test %>%
  select(-Petals)

set.seed(1234)
model.ctrl <- trainControl(method="repeatedcv",repeats = 3) #,classProbs=TRUE,summaryFunction = twoClassSummary)

model.knn <- train(Species ~ ., 
                   data = data.knn.train, 
                   trControl = model.ctrl,
                   preProcess = c("center","scale"), 
                   method = "knn")
model.knn
plot(model.knn)

predict.knn <- predict(model.knn,newdata = data.test )
#Get the confusion matrix to see accuracy value and other parameter values
confusionMatrix(predict.knn, data.test$Species )

# ========================================================
# random forests

model.rf <- train(Species ~ ., 
                data=data.train, 
                method="rf", 
                prox=TRUE)

model.rf
plot(model.rf)

predict.rf <- predict(model.rf, data.test)
confusionMatrix(predict.rf, data.test$Species )

# ========================================================
# decision tree model with important variables

set.seed(123)
model.ctrl <- trainControl(method = 'cv', number=6)

# minsplit=30, cp=0.001 (cp=cost complexity)
model.grid <- expand.grid(cp=seq(0, 0.05, 0.005))
model.formula <- Species ~ .
model.rpartCV <- train(model.formula, 
                           data=data.train,
                           method = 'rpart', 
                           trControl=model.ctrl, 
                           #                           metric='RMSE',
                           maximize=FALSE, 
                           tuneGrid = model.grid)

# results
summary(model.rpartCV)
plot(model.rpartCV)

# important variables
gbmImp.down <- varImp(model.rpartCV, scale = FALSE)
gbmImp.down

# ========================================================

crosstab(data.train$Species, data.train$Petal.Length,0,5)
crosstab(data.train$Species, data.train$Petal.Width,0,5)
crosstab(data.train$Species, data.train$Sepal.Length,0,5)
crosstab(data.train$Species, data.train$Sepal.Width,0,5)

# ========================================================
# boruta

set.seed(13)

# https://www.kaggle.com/jimthompson/boruta-feature-importance-analysis

# candidate.features <- setdiff(names(sample.df),c(ID.VAR,TARGET.VAR))
# data.type <- sapply(candidate.features,function(x){class(sample.df[[x]])})
# table(data.type)
# deterimine data types
# explanatory.attributes <- setdiff(names(sample.df),c(ID.VAR,TARGET.VAR))
# data.classes <- sapply(explanatory.attributes,function(x){class(sample.df[[x]])})
# # categorize data types in the data set?
# unique.classes <- unique(data.classes)
# attr.data.types <- lapply(unique.classes,function(x){names(data.classes[data.classes==x])})
# names(attr.data.types) <- unique.classes

# select candidate features
# make sure no missing values
data.bor <- data.in %>%
  select(-Species)

# select response variable as list
data.resp <- data.in$Species

# run boruta
bor.results <- Boruta(data.bor,
                      data.resp,
                      maxRuns=101,
                      doTrace=0)
bor.results

plot(bor.results)

# ========================================================
# https://machinelearningmastery.com/linear-classification-in-r/

# library(VGAM)
# 
# # fit model
# fit <- vglm(Species~., family=multinomial, data=data.train[,1:5])
# 
# # summarize the fit
# summary(fit)
# 
# # make predictions
# probabilities <- predict(fit, iris[,1:4], type="response")
# predictions <- apply(probabilities, 1, which.max)
# predictions[which(predictions=="1")] <- levels(iris$Species)[1]
# predictions[which(predictions=="2")] <- levels(iris$Species)[2]
# predictions[which(predictions=="3")] <- levels(iris$Species)[3]
# # summarize accuracy
# table(predictions, iris$Species)

# ========================================================
# Naive Bayes
library(klaR)

# train a naive bayes model
model.nb <- NaiveBayes(Species~., data=data.train)
# make predictions
x_test <- data.test[,1:4]
y_test <- data.test[,5]
model.pred <- predict(model.nb, x_test)
# summarize results
confusionMatrix(model.pred$class, y_test)

# caret ===========================
# define training control
model.ctrl <- trainControl(method="cv", number=10)
# fix the parameters of the algorithm
#model.grid <- expand.grid(.fL=c(0), .usekernel=c(FALSE))
# train the model
model.nb <- train(Species~., 
                  data=data.train[,1:5], 
                  trControl=model.ctrl, 
                  method="nb") 

model.pred <- predict(model.nb, data.test[,1:5])
# summarize results
model.pred <- data.frame(model.pred)
names(model.pred) <- c("Species")

confusionMatrix(model.pred$Species, data.test$Species)

# ========================================================
# regression model

model.lm <- train(Petal.Width ~ Petal.Length + Sepal.Length,
                  data = data.train,
                  method="lm")

print(model.lm)

pred.lm <- predict(model.lm, data.test[,1:4])
model.pred <- data.frame(pred.lm)
names(model.pred) <- c("Petal.Width")
model.pred$Orig <- data.test$Petal.Width
model.pred$Species <- data.test$Species

ggplot(data=model.pred, aes(Orig, Petal.Width, colour=Species)) + 
  geom_point() + 
  geom_smooth()

# ========================================================
